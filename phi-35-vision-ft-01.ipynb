{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1760524424014
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "#from azure.ai.ml import MLClient, Input, MpiDistribution, command\n",
        "from azure.ai.ml import MLClient, Input, Output, PyTorchDistribution, command\n",
        "from azure.ai.ml.entities import (\n",
        "    AmlCompute, Environment, BuildContext, Data,\n",
        "    ManagedOnlineEndpoint, ManagedOnlineDeployment, CodeConfiguration, OnlineRequestSettings\n",
        ")\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "import datetime\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(override=True)\n",
        "\n",
        "# Azure ML workspace configuration\n",
        "SUBSCRIPTION_ID = os.getenv(\"SUBSCRIPTION_ID\")\n",
        "RESOURCE_GROUP = os.getenv(\"RESOURCE_GROUP\")\n",
        "WORKSPACE_NAME = os.getenv(\"WORKSPACE_NAME\")\n",
        "COMPUTE_CLUSTER = \"demo-gpucluster01\"\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
        "\n",
        "\n",
        "# authentication via managed identity or service principal (no hard-coded creds)\n",
        "ml_client = MLClient(DefaultAzureCredential(), SUBSCRIPTION_ID, RESOURCE_GROUP, WORKSPACE_NAME)\n",
        "\n",
        "# ensure compute cluster exists or create it\n",
        "try:\n",
        "    ml_client.compute.get(COMPUTE_CLUSTER)\n",
        "except Exception:\n",
        "    print(\"demo-gpucluster01 was not found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "<h5>Prepare Environment</h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1760155085715
        }
      },
      "outputs": [],
      "source": [
        "os.makedirs(\"environment\", exist_ok=True)\n",
        "os.makedirs(\"src\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile ./environment/Dockerfile\n",
        "FROM mcr.microsoft.com/aifx/acpt/stable-ubuntu2204-cu118-py310-torch271:biweekly.202509.1\n",
        "\n",
        "# Install pip dependencies\n",
        "COPY requirements.txt .\n",
        "RUN pip install -r requirements.txt --no-cache-dir\n",
        "# Upgrade known vulnerable packages again just to be safe\n",
        "RUN pip install --upgrade \\\n",
        "    requests==2.32.4 \\\n",
        "    urllib3==2.5.0 \\\n",
        "    pillow==11.3.0 || true\n",
        "#ENV MAX_JOBS=32\n",
        "#RUN pip install flash-attn==2.4.2 --no-build-isolation\n",
        "\n",
        "# Repeat for other envs if applicable\n",
        "RUN /opt/conda/bin/pip install --upgrade \\\n",
        "    requests==2.32.4 \\\n",
        "    urllib3==2.5.0 \\\n",
        "    pillow==11.3.0 || true\n",
        "\n",
        "RUN /opt/conda/envs/ptca/bin/pip install --upgrade \\\n",
        "    requests==2.32.4 \\\n",
        "    urllib3==2.5.0 \\\n",
        "    pillow==11.3.0 || true\n",
        "\n",
        "# Inference requirements\n",
        "COPY --from=mcr.microsoft.com/azureml/o16n-base/python-assets:20230419.v1 /artifacts /var/\n",
        "RUN apt-get update && \\\n",
        "    apt-get install -y --no-install-recommends \\\n",
        "        libcurl4 \\\n",
        "        liblttng-ust1 \\\n",
        "        libunwind8 \\\n",
        "        libxml++2.6-2v5 \\\n",
        "        nginx-light \\\n",
        "        psmisc \\\n",
        "        rsyslog \\\n",
        "        runit \\\n",
        "        unzip && \\\n",
        "    apt-get clean && rm -rf /var/lib/apt/lists/* && \\\n",
        "    cp /var/configuration/rsyslog.conf /etc/rsyslog.conf && \\\n",
        "    cp /var/configuration/nginx.conf /etc/nginx/sites-available/app && \\\n",
        "    ln -sf /etc/nginx/sites-available/app /etc/nginx/sites-enabled/app && \\\n",
        "    rm -f /etc/nginx/sites-enabled/default\n",
        "\n",
        "RUN apt-get update && \\\n",
        "    apt-get install -y --only-upgrade \\\n",
        "        libpython3.10-stdlib \\\n",
        "        python3.10 \\\n",
        "        libpython3.10-minimal \\\n",
        "        python3.10-minimal \\\n",
        "        libpam0g \\\n",
        "        libpam-modules-bin \\\n",
        "        libpam-modules \\\n",
        "        libpam-runtime \\\n",
        "        sudo && \\\n",
        "    apt-get clean && rm -rf /var/lib/apt/lists/*\n",
        "\n",
        "ENV SVDIR=/var/runit\n",
        "ENV WORKER_TIMEOUT=400\n",
        "EXPOSE 5001 8883 8888\n",
        "\n",
        "# support Deepspeed launcher requirement of passwordless ssh login\n",
        "RUN apt-get update\n",
        "RUN apt-get install -y openssh-server openssh-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile ./environment/requirements.txt\n",
        "azureml-core==1.60.0.post1\n",
        "azureml-dataset-runtime==1.60.0\n",
        "azureml-defaults==1.60.0\n",
        "azure-ml==0.0.1\n",
        "azure-ml-component==0.9.18.post2\n",
        "azureml-mlflow==1.60.0.post1\n",
        "azureml-contrib-services==1.60.0\n",
        "azureml-contrib-services==1.60.0\n",
        "azureml-inference-server-http\n",
        "inference-schema\n",
        "MarkupSafe==2.1.2\n",
        "regex\n",
        "pybind11\n",
        "urllib3==2.5.0\n",
        "requests==2.32.4\n",
        "pillow==11.3.0\n",
        "transformers==4.54.1\n",
        "cryptography>=42.0.4\n",
        "aiohttp>=3.12.14\n",
        "py-spy==0.3.12\n",
        "debugpy~=1.6.3\n",
        "ipykernel~=6.0\n",
        "tensorboard\n",
        "psutil~=5.8.0\n",
        "matplotlib~=3.5.0\n",
        "tqdm~=4.66.3\n",
        "py-cpuinfo==5.0.0\n",
        "torch-tb-profiler~=0.4.0\n",
        "peft==0.11.1\n",
        "datasets\n",
        "#accelerate==0.30.1\n",
        "accelerate\n",
        "#deepspeed==0.15.1\n",
        "Levenshtein\n",
        "av>=12\n",
        "ninja\n",
        "#bitsandbytes==0.43.1\n",
        "trl>=0.20.0\n",
        "trackio\n",
        "pynvml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1760515294538
        }
      },
      "outputs": [],
      "source": [
        "env_name = \"env-phi35v-03\"\n",
        "docker_dir=\"./environment\"\n",
        "\n",
        "env_docker_image = Environment(\n",
        "    build=BuildContext(path=docker_dir),\n",
        "    name=env_name,\n",
        "    description=\"Environment created from a Docker context.\",\n",
        ")\n",
        "env_asset = ml_client.environments.create_or_update(env_docker_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "<h3>Prepare Dataset</h3>\n",
        "<br>Run the following command\n",
        "\n",
        "```bash\n",
        "python convert_ucf101.py --out_dir /path/to/converted_ucf101\n",
        "```\n",
        "<br>Then upload the folder to somewhere on default blobstore\n",
        "<br>In this script, it is referred to by the uri\n",
        "\"azureml://datastores/workspaceblobstore/paths/converted_ucf101/\"\n",
        "<br>It is possible to directly load data using \"Register Dataset\" cell below, but will get timeout."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "<h5>Preview Data</h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1760515322641
        }
      },
      "outputs": [],
      "source": [
        "# Preview converted UCF101 dataset (jsonl structure) and build label map\n",
        "import os, json, itertools\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_DIR = Path(\"./converted_ucf101\")  # local relative (mounted in AML job as well)\n",
        "if not DATA_DIR.exists():\n",
        "    # fallback to workspaceblobstore mount path in AML run context (user mode this may not resolve)\n",
        "    print(\"[WARN] Local converted_ucf101 not found. Ensure this path exists when running locally.\")\n",
        "\n",
        "train_file = DATA_DIR / \"ucf101_train.jsonl\"\n",
        "val_file = DATA_DIR / \"ucf101_val.jsonl\"\n",
        "print(\"Train file:\", train_file, \"Exists:\", train_file.exists())\n",
        "print(\"Val file:\", val_file, \"Exists:\", val_file.exists())\n",
        "\n",
        "example_lines = []\n",
        "if train_file.exists():\n",
        "    with open(train_file, 'r', encoding='utf-8') as f:\n",
        "        for _ in range(3):\n",
        "            line = f.readline()\n",
        "            if not line:\n",
        "                break\n",
        "            example_lines.append(json.loads(line))\n",
        "\n",
        "print(\"Sample converted entries (truncated):\")\n",
        "for ex in example_lines:\n",
        "    conv = ex['conversations'][0]\n",
        "    print({k: (str(v)[:80] + '...') if isinstance(v, str) else v for k, v in ex.items() if k != 'conversations'})\n",
        "    print(\" images:\", conv['images'][:3], \" user:\", conv['user'][:60], \" label:\", conv['assistant'])\n",
        "\n",
        "# Build class label list\n",
        "all_labels = []\n",
        "if train_file.exists():\n",
        "    with open(train_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            obj = json.loads(line)\n",
        "            all_labels.append(obj['conversations'][0]['assistant'])\n",
        "label_freq = Counter(all_labels)\n",
        "class_labels = sorted(label_freq.keys())\n",
        "label2id = {c:i for i,c in enumerate(class_labels)}\n",
        "id2label = {i:c for c,i in label2id.items()}\n",
        "print(f\"Classes ({len(class_labels)}):\", class_labels)\n",
        "print(\"Label freq (top 5):\", label_freq.most_common(5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "<h5>Register Dataset</h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1760516050893
        }
      },
      "outputs": [],
      "source": [
        "#data_uri = \"./converted_ucf101/\"\n",
        "data_uri = \"azureml://datastores/workspaceblobstore/paths/converted_ucf101/\"\n",
        "\n",
        "data = Data(\n",
        "    path = data_uri,\n",
        "    type = AssetTypes.URI_FOLDER,\n",
        "    description = \"phi35vft-ucf101\",\n",
        "    name = \"phi35vft-ucf101-02\",\n",
        "    version = '1'\n",
        ")\n",
        "ml_client.data.create_or_update(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "<h5>Data Collators</h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile ./src/phi3v_dataset.py\n",
        "import copy\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "IGNORE_INDEX = -100\n",
        "\n",
        "\n",
        "def pad_sequence(sequences, padding_side='right', padding_value=0):\n",
        "    \"\"\"Pad a list of 1D tensors to the same length and stack.\"\"\"\n",
        "    assert padding_side in ['right', 'left']\n",
        "    max_len = max(seq.size(0) for seq in sequences)\n",
        "    batch_size = len(sequences)\n",
        "    device = sequences[0].device\n",
        "    dtype = sequences[0].dtype\n",
        "    out = torch.full((batch_size, max_len), padding_value, dtype=dtype, device=device)\n",
        "    for i, seq in enumerate(sequences):\n",
        "        length = seq.size(0)\n",
        "        if padding_side == 'right':\n",
        "            out[i, :length] = seq\n",
        "        else:\n",
        "            out[i, -length:] = seq\n",
        "    return out\n",
        "\n",
        "\n",
        "class Phi3VDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A minimal implementation that stays as close as possible to the original PhiCookBook structure.\n",
        "    - Processes one example in __getitem__ (image → prompt → tokenization)\n",
        "    - Converts input_ids / labels to list[int] (to avoid tensor-based truth value errors in TRL's truncate_dataset)\n",
        "    - If max_seq_length is specified, performs a simple trim keeping the right end\n",
        "    - map() is a no-op to satisfy TRL's .map call (within truncate_dataset)\n",
        "    → Column-based operations are disabled since this is not a Hugging Face datasets.Dataset\n",
        "    \"\"\"\n",
        "    def __init__(self, jsonl_file: str, image_dir: str, processor, max_seq_length: int | None = None):\n",
        "        self.image_dir = Path(image_dir)\n",
        "        with open(jsonl_file, 'r', encoding='utf-8') as f:\n",
        "            self.examples = [json.loads(line) for line in f]\n",
        "        self.processor = processor\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def shard(self, num_shards, shard_id):\n",
        "        sharded = copy.deepcopy(self)\n",
        "        sharded.examples = [self.examples[i] for i in range(shard_id, len(self.examples), num_shards)]\n",
        "        return sharded\n",
        "\n",
        "    def _get_inputs(self, user_text, image_paths):\n",
        "        images = [Image.open(self.image_dir / image_path) for image_path in image_paths]\n",
        "        image_tag_text = ''.join([f'<|image_{i}|>' for i in range(1, len(images) + 1)])\n",
        "        prompt_message = {'role': 'user', 'content': f'{image_tag_text}\\n{user_text}'}\n",
        "        prompt = self.processor.tokenizer.apply_chat_template(\n",
        "            [prompt_message], tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "        inputs = self.processor(prompt, images, return_tensors='pt')\n",
        "        return inputs\n",
        "\n",
        "    def _truncate_pair(self, ids: torch.Tensor, labels: torch.Tensor):\n",
        "        if self.max_seq_length is None:\n",
        "            return ids, labels\n",
        "        if ids.size(0) <= self.max_seq_length:\n",
        "            return ids, labels\n",
        "        return ids[-self.max_seq_length:], labels[-self.max_seq_length:]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        example = self.examples[idx]\n",
        "        all_input_ids = []\n",
        "        all_labels = []\n",
        "        all_pixel_values = []\n",
        "        all_image_sizes = []\n",
        "\n",
        "        for turn in example['conversations']:\n",
        "            inputs = self._get_inputs(turn['user'], turn['images'])\n",
        "            prompt_input_ids = inputs['input_ids']  # (1, P)\n",
        "            assistant_text = turn['assistant']\n",
        "            response = f'{assistant_text}<|end|>\\n<|endoftext|>'\n",
        "            response_input_ids = self.processor.tokenizer(\n",
        "                response, add_special_tokens=False, return_tensors='pt'\n",
        "            )['input_ids']  # (1, R)\n",
        "\n",
        "            input_ids = torch.cat([prompt_input_ids, response_input_ids], dim=1).squeeze(0)\n",
        "            labels = torch.cat([\n",
        "                torch.full((prompt_input_ids.size(1),), IGNORE_INDEX, dtype=torch.long),\n",
        "                response_input_ids.squeeze(0)\n",
        "            ], dim=0)\n",
        "\n",
        "            all_input_ids.append(input_ids)\n",
        "            all_labels.append(labels)\n",
        "            all_pixel_values.append(inputs['pixel_values'])\n",
        "            all_image_sizes.append(inputs['image_sizes'])\n",
        "\n",
        "        input_ids = torch.cat(all_input_ids, dim=0)\n",
        "        labels = torch.cat(all_labels, dim=0)\n",
        "        input_ids, labels = self._truncate_pair(input_ids, labels)\n",
        "        pixel_values = torch.cat(all_pixel_values, dim=0)\n",
        "        image_sizes = torch.cat(all_image_sizes, dim=0)\n",
        "\n",
        "        return {\n",
        "            'id': example['id'],\n",
        "            'input_ids': input_ids.tolist(),   # list[int]\n",
        "            'labels': labels.tolist(),         # list[int]\n",
        "            'pixel_values': pixel_values,      # tensor\n",
        "            'image_sizes': image_sizes,        # tensor\n",
        "        }\n",
        "\n",
        "    # Required for TRL's truncate_dataset which calls dataset.map(truncate, ...).\n",
        "    # Since this is not a Hugging Face datasets.Dataset, simply return self without doing anything.\n",
        "    def map(self, function, *args, **kwargs):  # pragma: no cover\n",
        "        return self\n",
        "\n",
        "\n",
        "class Phi3VDataCollator:\n",
        "    def __init__(self, pad_token_id: int):\n",
        "        self.pad_token_id = pad_token_id\n",
        "\n",
        "    def __call__(self, examples):\n",
        "        batch_input_ids = [torch.tensor(ex['input_ids'], dtype=torch.long) for ex in examples]\n",
        "        batch_label_ids = [torch.tensor(ex['labels'], dtype=torch.long) for ex in examples]\n",
        "        batch_pixel_values = [ex['pixel_values'] for ex in examples]\n",
        "        batch_image_sizes = [ex['image_sizes'] for ex in examples]\n",
        "\n",
        "        input_ids = pad_sequence(batch_input_ids, padding_side='right', padding_value=self.pad_token_id)\n",
        "        attention_mask = input_ids != self.pad_token_id\n",
        "        labels = pad_sequence(batch_label_ids, padding_side='right', padding_value=IGNORE_INDEX)\n",
        "        pixel_values = torch.cat(batch_pixel_values, dim=0)\n",
        "        image_sizes = torch.cat(batch_image_sizes, dim=0)\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'labels': labels,\n",
        "            'attention_mask': attention_mask,\n",
        "            'pixel_values': pixel_values,\n",
        "            'image_sizes': image_sizes,\n",
        "        }\n",
        "\n",
        "\n",
        "class Phi3VEvalDataset(Phi3VDataset):\n",
        "    def __getitem__(self, idx):\n",
        "        example = self.examples[idx]\n",
        "        messages = []\n",
        "        all_images = []\n",
        "        for i, turn in enumerate(example['conversations']):\n",
        "            images = [Image.open(self.image_dir / image_path) for image_path in turn['images']]\n",
        "            image_tag_text = ''.join([f'<|image_{i}|>' for i in range(1, len(images) + 1)])\n",
        "            prompt_message = {'role': 'user', 'content': f\"{image_tag_text}\\n{turn['user']}\"}\n",
        "            messages.append(prompt_message)\n",
        "            all_images.extend(images)\n",
        "            if i + 1 == len(example['conversations']):\n",
        "                break\n",
        "            response_message = {'role': 'assistant', 'content': f\"{turn['assistant']}<|end|>\\n<|endoftext|>\"}\n",
        "            messages.append(response_message)\n",
        "        prompt = self.processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "        inputs = self.processor(prompt, all_images, return_tensors='pt')\n",
        "        answer = example['conversations'][-1].get('assistant')\n",
        "        return {\n",
        "            'id': example['id'],\n",
        "            'input_ids': inputs['input_ids'].squeeze(0).tolist(),\n",
        "            'pixel_values': inputs['pixel_values'],\n",
        "            'image_sizes': inputs['image_sizes'],\n",
        "            'answer': answer,\n",
        "        }\n",
        "\n",
        "\n",
        "class Phi3VEvalDataCollator(Phi3VDataCollator):\n",
        "    def __call__(self, examples):\n",
        "        unique_ids = [ex['id'] for ex in examples]\n",
        "        batch_input_ids = [torch.tensor(ex['input_ids'], dtype=torch.long) for ex in examples]\n",
        "        batch_pixel_values = [ex['pixel_values'] for ex in examples]\n",
        "        batch_image_sizes = [ex['image_sizes'] for ex in examples]\n",
        "        answers = [ex['answer'] for ex in examples]\n",
        "\n",
        "        input_ids = pad_sequence(batch_input_ids, padding_side='left', padding_value=self.pad_token_id)\n",
        "        attention_mask = input_ids != self.pad_token_id\n",
        "        pixel_values = torch.cat(batch_pixel_values, dim=0)\n",
        "        image_sizes = torch.cat(batch_image_sizes, dim=0)\n",
        "\n",
        "        return {\n",
        "            'unique_ids': unique_ids,\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'pixel_values': pixel_values,\n",
        "            'image_sizes': image_sizes,\n",
        "            'answers': answers,\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "<h5>Processor Patch</h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile ./src/processor_patch.py\n",
        "\"\"\"\n",
        "Processor patch utilities for robust save_pretrained behavior.\n",
        "\n",
        "Why this exists:\n",
        "- Upstream AutoProcessor for multimodal Phi models occasionally references optional\n",
        "  attributes (e.g., chat_template, audio_tokenizer, video_processor) that may not be\n",
        "  present depending on model / transformers version.\n",
        "- During Trainer checkpoint saves, AttributeError would abort training unless we guard it.\n",
        "\n",
        "Usage:\n",
        "    from processor_patch import patch_processor_save\n",
        "    processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
        "    patch_processor_save(processor)\n",
        "\n",
        "Extension:\n",
        "- If new optional attributes appear upstream, append them to OPTIONAL_ATTRS below.\n",
        "- For silent mode (suppress warnings), set env PHI_PROC_PATCH_SILENT=1.\n",
        "\"\"\"\n",
        "from __future__ import annotations\n",
        "import os\n",
        "from typing import List, Optional\n",
        "\n",
        "OPTIONAL_ATTRS: List[str] = [\n",
        "    \"chat_template\",\n",
        "    \"audio_tokenizer\",\n",
        "    \"video_processor\",\n",
        "]\n",
        "\n",
        "WARN_PREFIX = \"[PROC-PATCH]\"\n",
        "\n",
        "\n",
        "def _log(msg: str):  # minimal logging helper\n",
        "    if os.getenv(\"PHI_PROC_PATCH_SILENT\"):\n",
        "        return\n",
        "    print(f\"{WARN_PREFIX} {msg}\")\n",
        "\n",
        "\n",
        "def ensure_optional_placeholders(processor, extra: Optional[List[str]] = None):\n",
        "    attrs = OPTIONAL_ATTRS + (extra or [])\n",
        "    for attr in attrs:\n",
        "        if not hasattr(processor, attr):\n",
        "            setattr(processor, attr, None)\n",
        "    return processor\n",
        "\n",
        "\n",
        "def safe_save_processor_components(processor, out_dir: str):\n",
        "    # tokenizer\n",
        "    tok = getattr(processor, \"tokenizer\", None)\n",
        "    if tok is not None:\n",
        "        try:\n",
        "            tok.save_pretrained(out_dir)\n",
        "        except Exception as e:\n",
        "            _log(f\"tokenizer save failed: {e}\")\n",
        "    # possible vision components\n",
        "    for attr in [\"image_processor\", \"feature_extractor\", \"vision_processor\"]:\n",
        "        comp = getattr(processor, attr, None)\n",
        "        if comp is not None:\n",
        "            try:\n",
        "                comp.save_pretrained(out_dir)\n",
        "            except Exception as e:\n",
        "                _log(f\"{attr} save failed: {e}\")\n",
        "\n",
        "\n",
        "def patch_processor_save(processor, optional_attrs: Optional[List[str]] = None, verbose: bool = True):\n",
        "    \"\"\"Monkeypatch processor.save_pretrained to be resilient.\n",
        "\n",
        "    1. Ensure optional attrs exist (set to None if missing)\n",
        "    2. Wrap original save_pretrained; on AttributeError (or generic Exception) fall back\n",
        "       to component-wise saves.\n",
        "    \"\"\"\n",
        "    ensure_optional_placeholders(processor, optional_attrs)\n",
        "    original = getattr(processor, \"save_pretrained\", None)\n",
        "\n",
        "    def _safe_save_pretrained(save_directory, *args, **kwargs):\n",
        "        if original is not None:\n",
        "            try:\n",
        "                return original(save_directory, *args, **kwargs)\n",
        "            except AttributeError as e:\n",
        "                if verbose:\n",
        "                    _log(f\"AttributeError caught: {e}; falling back to component saves.\")\n",
        "            except Exception as e:  # catch-all so training never aborts on save\n",
        "                if verbose:\n",
        "                    _log(f\"{type(e).__name__} during save_pretrained: {e}; falling back to component saves.\")\n",
        "        safe_save_processor_components(processor, save_directory)\n",
        "\n",
        "    processor.save_pretrained = _safe_save_pretrained  # type: ignore\n",
        "    if verbose:\n",
        "        _log(\"processor.save_pretrained patched (resilient mode enabled)\")\n",
        "    return processor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "<h5> Training code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756866757124
        }
      },
      "outputs": [],
      "source": [
        "%%writefile ./src/train.py\n",
        "\"\"\"\n",
        "Minimal Phi-3.5 Vision LoRA fine-tuning script (UCF101) aligned closely with PhiCookBook original.\n",
        "- Uses TRL SFTTrainer\n",
        "- Static LoRA target modules (language + vision)\n",
        "- Optional bf16 (preferred) fallback to fp16\n",
        "- Keeps remove_unused_columns=False for multimodal batches\n",
        "- Merges LoRA adapter into base model after training (for simplified deployment)\n",
        "\n",
        "Processor save resilience is externalized to processor_patch.py to keep this script focused.\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from transformers import AutoProcessor, AutoModelForCausalLM\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from trl import SFTConfig, SFTTrainer\n",
        "\n",
        "from processor_patch import patch_processor_save  # externalized monkeypatch\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument('--data_dir', type=str, required=True, help='Folder with ucf101_train.jsonl / ucf101_val.jsonl / images/')\n",
        "    p.add_argument('--train_file', type=str, default='ucf101_train.jsonl')\n",
        "    p.add_argument('--val_file', type=str, default='ucf101_val.jsonl')\n",
        "    p.add_argument('--model_name', type=str, default='microsoft/phi-3.5-vision-instruct')\n",
        "    p.add_argument('--output_dir', type=str, default='./outputs')\n",
        "    p.add_argument('--epochs', type=int, default=1)\n",
        "    p.add_argument('--lr', type=float, default=5e-5)\n",
        "    p.add_argument('--warmup_ratio', type=float, default=0.03)\n",
        "    p.add_argument('--batch_size', type=int, default=1)\n",
        "    p.add_argument('--grad_accum', type=int, default=16)\n",
        "    p.add_argument('--logging_steps', type=int, default=25)\n",
        "    p.add_argument('--save_steps', type=int, default=200)\n",
        "    p.add_argument('--lora_r', type=int, default=16)\n",
        "    p.add_argument('--lora_alpha', type=int, default=32)\n",
        "    p.add_argument('--lora_dropout', type=float, default=0.05)\n",
        "    p.add_argument('--seed', type=int, default=42)\n",
        "    p.add_argument('--bf16', action='store_true', help='Use bfloat16 if supported, else fallback to fp16.')\n",
        "    return p.parse_args()\n",
        "\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    import random, numpy as np\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def select_dtype(use_bf16: bool):\n",
        "    if not torch.cuda.is_available():\n",
        "        raise RuntimeError('CUDA GPU required for this vision fine-tuning script.')\n",
        "    if use_bf16 and torch.cuda.is_bf16_supported():\n",
        "        return torch.bfloat16\n",
        "    return torch.float16\n",
        "\n",
        "\n",
        "def create_lora_config(rank: int, alpha: int, dropout: float):\n",
        "    # Close to cookbook: explicit module list (language + vision)\n",
        "    target_modules = [\n",
        "        # language side\n",
        "        'qkv_proj', 'o_proj', 'down_proj', 'gate_up_proj', 'lm_head',\n",
        "        # vision side (attention + MLP + projection)\n",
        "        'q_proj', 'k_proj', 'v_proj', 'out_proj', 'fc1', 'fc2', 'img_projection.0', 'img_projection.2'\n",
        "    ]\n",
        "    return LoraConfig(\n",
        "        r=rank,\n",
        "        lora_alpha=alpha,\n",
        "        lora_dropout=dropout,\n",
        "        target_modules=target_modules,\n",
        "    )\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "    set_seed(args.seed)\n",
        "\n",
        "    torch_dtype = select_dtype(args.bf16)\n",
        "    print(f\"[INFO] Using dtype={torch_dtype}\")\n",
        "\n",
        "    # Load processor & base model\n",
        "    processor = AutoProcessor.from_pretrained(args.model_name, trust_remote_code=True)\n",
        "    patch_processor_save(processor)  # install resilient save behavior\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        args.model_name,\n",
        "        attn_implementation='flash_attention_2',  # assumes flash-attn installed via environment or launcher\n",
        "        torch_dtype=torch_dtype,\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "\n",
        "    # Apply LoRA\n",
        "    lora_cfg = create_lora_config(\n",
        "        args.lora_r,\n",
        "        args.lora_alpha,\n",
        "        args.lora_dropout\n",
        "        )\n",
        "    print(f\"[INFO] LoRA target modules ({len(lora_cfg.target_modules)}): {lora_cfg.target_modules}\")\n",
        "\n",
        "    model = get_peft_model(model, lora_cfg)\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    # Datasets\n",
        "    data_dir = Path(args.data_dir)\n",
        "    train_path = data_dir / args.train_file\n",
        "    val_path = data_dir / args.val_file\n",
        "\n",
        "    from phi3v_dataset import Phi3VDataset, Phi3VDataCollator\n",
        "    print(f\"[INFO] Loading train dataset: {train_path}\")\n",
        "    train_ds = Phi3VDataset(str(train_path), str(data_dir / 'images'), processor)\n",
        "    eval_ds = Phi3VDataset(str(val_path), str(data_dir / 'images'), processor) if val_path.exists() else None\n",
        "\n",
        "    pad_token_id = processor.tokenizer.pad_token_id or processor.tokenizer.eos_token_id\n",
        "    collator = Phi3VDataCollator(pad_token_id=pad_token_id)\n",
        "\n",
        "    training_args = SFTConfig(\n",
        "        output_dir=args.output_dir,\n",
        "        num_train_epochs=args.epochs,\n",
        "        per_device_train_batch_size=args.batch_size,\n",
        "        gradient_accumulation_steps=args.grad_accum,\n",
        "        learning_rate=args.lr,\n",
        "        warmup_ratio=args.warmup_ratio,\n",
        "        logging_steps=args.logging_steps,\n",
        "        save_steps=args.save_steps,\n",
        "        save_total_limit=2,\n",
        "        bf16=(torch_dtype == torch.bfloat16),\n",
        "        fp16=(torch_dtype == torch.float16),\n",
        "        gradient_checkpointing=True,\n",
        "        gradient_checkpointing_kwargs={'use_reentrant': False},\n",
        "        ddp_find_unused_parameters=False,\n",
        "        report_to=\"trackio\", # changed from none\n",
        "        remove_unused_columns=False,  # keep pixel_values, image_sizes\n",
        "    )\n",
        "\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=eval_ds,\n",
        "        data_collator=collator,\n",
        "        processing_class=processor,  # safe due to patch\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    print('[INFO] Saving final adapter & processor ...')\n",
        "    trainer.model.save_pretrained(args.output_dir)\n",
        "    processor.save_pretrained(args.output_dir)\n",
        "\n",
        "    # Merge LoRA adapter into base model for simplified deployment\n",
        "    try:\n",
        "        print('[INFO] Merging LoRA adapter into base model...')\n",
        "        merged_model = trainer.model.merge_and_unload()\n",
        "        merged_dir = Path(args.output_dir) / \"merged\"\n",
        "        merged_dir.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        # Use PyTorch bin format to handle tied weights (embed_tokens <-> vision_embed_tokens.wte)\n",
        "        merged_model.save_pretrained(str(merged_dir), safe_serialization=False)\n",
        "        processor.save_pretrained(str(merged_dir))\n",
        "        print(f'[INFO] Merged model saved at: {merged_dir}')\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        print(f'[WARN] Could not merge model automatically: {e}')\n",
        "        traceback.print_exc()\n",
        "        print('[WARN] Adapter-only checkpoint still available in output_dir.')\n",
        "\n",
        "    print('[INFO] Done.')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "<h5>Training shell</h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile ./src/train.sh\n",
        "#!/usr/bin/env bash\n",
        "# Lightweight launcher that installs flash-attn (if not already present) then delegates to train.py\n",
        "# Usage: bash train.sh --data_dir <path> --train_file <jsonl> --val_file <jsonl> [other train.py args]\n",
        "# You can still run python train.py directly via AML command() if you do not need flash-attn.\n",
        "\n",
        "set -euxo pipefail\n",
        "\n",
        "# Adjust CUDA_TAG if you change the base image (e.g. cu121 for CUDA 12.1). Default aligns with current base image.\n",
        "CUDA_TAG=${CUDA_TAG:-cu118}\n",
        "\n",
        "# Limit parallel build jobs to reduce memory pressure on shared GPU nodes.\n",
        "export MAX_JOBS=$(python -c \"import os; n=min(8, os.cpu_count() or 8); print(n)\")\n",
        "\n",
        "echo \"[INFO] Using CUDA_TAG=${CUDA_TAG} MAX_JOBS=${MAX_JOBS}\" \n",
        "\n",
        "LOCK=/tmp/train_install.lock\n",
        "flock -x \"$LOCK\" bash -c '\n",
        "# Ensure recent pip & ninja (ninja accelerates any light builds triggered by wheel fallbacks)\n",
        "python -m pip install -U pip ninja\n",
        "\n",
        "# Install flash-attn (idempotent). Prefer pre-built wheels from vendor index.\n",
        "python -m pip install --no-build-isolation --prefer-binary \\\n",
        "  --extra-index-url https://flash-attn.ai/whl/${CUDA_TAG} \\\n",
        "  flash-attn==2.4.2 || echo \"[WARN] flash-attn install failed or already satisfied\"\n",
        "\n",
        "python -m pip install -U pip\n",
        "python -m pip install --prefer-binary deepspeed==0.15.1 || true\n",
        "'\n",
        "\n",
        "echo \"[INFO] Launching training (delegating to train.py)\"\n",
        "python train.py \"$@\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "<h5>Submit Job</h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1760517345822
        }
      },
      "outputs": [],
      "source": [
        "# AML job configuration for Phi-3.5 Vision fine-tuning on UCF101\n",
        "NUM_NODES = 1  # Start single node; scale after validation\n",
        "NUM_GPU_PER_NODE = 1\n",
        "\n",
        "# Distributed config (can expand to multi-node later)\n",
        "dist = PyTorchDistribution(\n",
        "    process_count_per_instance=NUM_GPU_PER_NODE,\n",
        "    node_count=NUM_NODES\n",
        ")\n",
        "\n",
        "# Data asset folder created earlier: phi35vft-ucf101 version 1 contains jsonl + images\n",
        "# We mount the folder and point --data_dir to the mount root.\n",
        "\n",
        "vision_job = command(\n",
        "    code=\"./src\",\n",
        "    command=(\n",
        "        \"bash -lc \"\n",
        "        \"'./train.sh \"\n",
        "        #\"python train.py \"\n",
        "        \"--data_dir ${{inputs.data_dir}} \"\n",
        "        \"--train_file ucf101_train.jsonl \"\n",
        "        \"--val_file ucf101_val.jsonl \"\n",
        "        \"--model_name microsoft/phi-3.5-vision-instruct \"\n",
        "        \"--epochs 1 \"\n",
        "        \"--batch_size 1 \"\n",
        "        \"--grad_accum 16 \"\n",
        "        \"--lr 5e-5 \"\n",
        "        \"--warmup_ratio 0.03 \"\n",
        "        \"--save_steps 200 \"\n",
        "        \"--logging_steps 5 \"\n",
        "        \"--bf16 '\"  # model script enforces flash-attn with half precision\"\n",
        "    ),\n",
        "    inputs={\n",
        "        \"data_dir\": Input(\n",
        "            type=AssetTypes.URI_FOLDER,\n",
        "            path=\"phi35vft-ucf101:1\"  # Explicit version\n",
        "        )\n",
        "    },\n",
        "    outputs={                         # not used here\n",
        "        \"model_dir\": Output(\n",
        "            type=AssetTypes.URI_FOLDER,\n",
        "            mode=\"rw_mount\",\n",
        "            path=\"azureml://datastores/workspaceblobstore/paths/models/phi35v-ucf101\"\n",
        "        )\n",
        "    },\n",
        "    environment=\"env-phi35v-03:2\",  # Built earlier in this notebook\n",
        "    compute=COMPUTE_CLUSTER,\n",
        "    display_name=\"phi35v-vision-ucf101-ft\",\n",
        "    experiment_name=\"phi35v-vision-ft-exp\",\n",
        "    instance_count=NUM_NODES,\n",
        "    distribution=dist,\n",
        "    environment_variables={\n",
        "        \"NCCL_DEBUG\": \"WARN\",\n",
        "        \"PYTORCH_CUDA_ALLOC_CONF\": \"expandable_segments:True\",\n",
        "        \"HF_TOKEN\": HF_TOKEN or \"\",\n",
        "        \"HF_HOME\": \"./outputs/hfhome\",\n",
        "        \"TRACKIO_PROJECT\": \"ft-project\",\n",
        "    }\n",
        ")\n",
        "returned_job = ml_client.jobs.create_or_update(vision_job)\n",
        "print(returned_job.studio_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "<h5> Register Model </h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1760521843621
        }
      },
      "outputs": [],
      "source": [
        "model_path_from_job = \"azureml://jobs/{0}/outputs/{1}\".format(\n",
        "    returned_job.name, \"artifacts/outputs/merged\"\n",
        ")\n",
        "\n",
        "print(\"path to register model: \", model_path_from_job)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1760524810764
        }
      },
      "outputs": [],
      "source": [
        "#model_path_from_job = \"azureml://jobs/dynamic_reggae_snc3rdfp9r/outputs/artifacts/outputs/merged\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1760524814228
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import Model\n",
        "model = Model(\n",
        "    path=model_path_from_job,  # Reference to training job output\n",
        "    name=\"phi-35-vision-ucf101\",  # Model name in registry\n",
        "    description=\"LoRA fine-tuned phi-35-vision model for UCF101 dataset\",\n",
        "    type=\"custom_model\",   # Hugging Face models are commonly registered as custom_model\n",
        "    version=\"2\",             # Explicit version (or omit for auto increment)\n",
        ")\n",
        "\n",
        "registered_model = ml_client.models.create_or_update(model)\n",
        "print(f\"Registered: {registered_model.name}:{registered_model.version}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "<h5> Score.py </h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile ./src/score.py\n",
        "\"\"\"\n",
        "Phi-3.5 Vision UCF101 Action Classification Inference Script\n",
        "Handles multi-frame video classification using fine-tuned merged model.\n",
        "\"\"\"\n",
        "import os\n",
        "import json\n",
        "import base64\n",
        "from io import BytesIO\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import AutoProcessor, AutoModelForCausalLM\n",
        "\n",
        "\n",
        "# UCF101 class list (10 classes as shown in test data)\n",
        "UCF101_CLASSES = [\n",
        "    \"ApplyEyeMakeup\", \"ApplyLipstick\", \"Archery\", \"BabyCrawling\", \"BalanceBeam\",\n",
        "    \"BandMarching\", \"BaseballPitch\", \"Basketball\", \"BasketballDunk\", \"BenchPress\"\n",
        "]\n",
        "\n",
        "\n",
        "def init():\n",
        "    \"\"\"Initialize model and processor from merged checkpoint.\"\"\"\n",
        "    global model, processor\n",
        "    \n",
        "    model_root = os.environ.get(\"AZUREML_MODEL_DIR\", \".\")\n",
        "    model_dir = os.path.join(model_root, os.getenv(\"MODEL_SUBDIR\", \"merged\"))\n",
        "    \n",
        "    print(f\"[INFO] Loading model from: {model_dir}\")\n",
        "    \n",
        "    # Load processor (handles tokenizer + image_processor)\n",
        "    processor = AutoProcessor.from_pretrained(\n",
        "        model_dir,\n",
        "        trust_remote_code=True,\n",
        "        local_files_only=True\n",
        "    )\n",
        "    \n",
        "    # Load merged model (base + LoRA already fused)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_dir,\n",
        "        trust_remote_code=True,\n",
        "        torch_dtype=(torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16),\n",
        "        device_map=(\"auto\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        local_files_only=True,\n",
        "    )\n",
        "    model.eval()\n",
        "    print(f\"[INFO] Model loaded successfully on device: {model.device}\")\n",
        "\n",
        "\n",
        "def _load_images_from_request(data):\n",
        "    \"\"\"\n",
        "    Load images from request payload.\n",
        "    Supports:\n",
        "    - \"image_paths\": list of file paths (for local testing)\n",
        "    - \"images\": list of base64-encoded image strings\n",
        "    - \"image\": single base64-encoded image string\n",
        "    \"\"\"\n",
        "    images = []\n",
        "    \n",
        "    # Option 1: Multiple image paths (local file testing)\n",
        "    if \"image_paths\" in data:\n",
        "        for path in data[\"image_paths\"]:\n",
        "            images.append(Image.open(path).convert(\"RGB\"))\n",
        "    \n",
        "    # Option 2: Multiple base64 images\n",
        "    elif \"images\" in data:\n",
        "        for img_b64 in data[\"images\"]:\n",
        "            img_bytes = base64.b64decode(img_b64)\n",
        "            images.append(Image.open(BytesIO(img_bytes)).convert(\"RGB\"))\n",
        "    \n",
        "    # Option 3: Single base64 image (fallback)\n",
        "    elif \"image\" in data:\n",
        "        img_bytes = base64.b64decode(data[\"image\"])\n",
        "        images.append(Image.open(BytesIO(img_bytes)).convert(\"RGB\"))\n",
        "    \n",
        "    else:\n",
        "        raise ValueError(\"Request must contain 'image_paths', 'images', or 'image' field\")\n",
        "    \n",
        "    return images\n",
        "\n",
        "\n",
        "def run(raw_data):\n",
        "    \"\"\"\n",
        "    Run inference on UCF101 action classification.\n",
        "    \n",
        "    Expected input format:\n",
        "    {\n",
        "        \"images\": [\"<base64>\", \"<base64>\", ...],  # or \"image_paths\" for local test\n",
        "        \"prompt\": \"Classify the video...\",  # optional, uses default if not provided\n",
        "        \"max_new_tokens\": 50  # optional\n",
        "    }\n",
        "    \n",
        "    Returns:\n",
        "    {\n",
        "        \"predicted_class\": \"ApplyLipstick\",\n",
        "        \"raw_output\": \"ApplyLipstick\"\n",
        "    }\n",
        "    \"\"\"\n",
        "    try:\n",
        "        data = json.loads(raw_data) if isinstance(raw_data, str) else raw_data\n",
        "        \n",
        "        # Load images\n",
        "        images = _load_images_from_request(data)\n",
        "        print(f\"[INFO] Loaded {len(images)} images for inference\")\n",
        "        \n",
        "        # Build prompt (default to UCF101 classification task)\n",
        "        user_prompt = data.get(\n",
        "            \"prompt\",\n",
        "            f\"Classify the video into one of the following classes: {', '.join(UCF101_CLASSES)}.\"\n",
        "        )\n",
        "        \n",
        "        # Build image tags for multi-frame input\n",
        "        image_tag_text = ''.join([f'<|image_{i}|>' for i in range(1, len(images) + 1)])\n",
        "        \n",
        "        # Create user message with chat template\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": f\"{image_tag_text}\\n{user_prompt}\"}\n",
        "        ]\n",
        "        \n",
        "        # Apply chat template\n",
        "        prompt = processor.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "        \n",
        "        # Preprocess (tokenize + vision encoding)\n",
        "        inputs = processor(prompt, images, return_tensors=\"pt\")\n",
        "        \n",
        "        # Move to device\n",
        "        inputs = {k: v.to(model.device) if isinstance(v, torch.Tensor) else v \n",
        "                  for k, v in inputs.items()}\n",
        "        \n",
        "        # Generation config\n",
        "        max_new_tokens = int(data.get(\"max_new_tokens\", 50))\n",
        "        \n",
        "        # Generate (use_cache=False to avoid DynamicCache.seen_tokens AttributeError)\n",
        "        with torch.no_grad():\n",
        "            output_ids = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                do_sample=False,  # Greedy for classification\n",
        "                use_cache=False,  # Disable KV cache to avoid trust_remote_code compatibility issues\n",
        "                pad_token_id=processor.tokenizer.pad_token_id or processor.tokenizer.eos_token_id,\n",
        "            )\n",
        "        \n",
        "        # Decode (remove prompt echo)\n",
        "        input_len = inputs[\"input_ids\"].shape[1]\n",
        "        generated_ids = output_ids[0, input_len:]\n",
        "        raw_output = processor.tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
        "        \n",
        "        # Extract predicted class (simple heuristic: first matching class name)\n",
        "        predicted_class = None\n",
        "        for cls in UCF101_CLASSES:\n",
        "            if cls.lower() in raw_output.lower():\n",
        "                predicted_class = cls\n",
        "                break\n",
        "        \n",
        "        return json.dumps({\n",
        "            \"predicted_class\": predicted_class,\n",
        "            \"raw_output\": raw_output\n",
        "        }, ensure_ascii=False)\n",
        "    \n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        error_trace = traceback.format_exc()\n",
        "        print(f\"[ERROR] {error_trace}\")\n",
        "        return json.dumps({\n",
        "            \"error\": str(e),\n",
        "            \"traceback\": error_trace\n",
        "        }, ensure_ascii=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "<h5> Create Endpoint </h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1760504666099
        }
      },
      "outputs": [],
      "source": [
        "endpoint_name = f\"phi35v-jp-{datetime.datetime.now():%m%d%H%M}\"\n",
        "\n",
        "# 1) Create endpoint\n",
        "endpoint = ManagedOnlineEndpoint(\n",
        "    name=endpoint_name, \n",
        "    description=\"phi-3.5-vision UCF101 demo endpoint\",\n",
        "    auth_mode=\"key\")\n",
        "ml_client.begin_create_or_update(endpoint).result()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "<h5> Deploy Model </h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1760524953369
        }
      },
      "outputs": [],
      "source": [
        "#endpoint_name=\"phi35v-jp-10150503\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1760525194458
        }
      },
      "outputs": [],
      "source": [
        "# 2) Deploy (refer to the already registered \"merged\" model in Model Registry)\n",
        "registered = ml_client.models.get(\"phi-35-vision-ucf101\", version=\"2\")\n",
        "\n",
        "deploy = ManagedOnlineDeployment(\n",
        "    name=\"blue\",\n",
        "    endpoint_name=endpoint_name,\n",
        "    model=registered.id,\n",
        "    environment=\"env-phi35v-01a@latest\",\n",
        "    code_configuration=CodeConfiguration(code=\"./src\", scoring_script=\"score.py\"),\n",
        "    instance_type=\"Standard_NC40ads_H100_v5\", \n",
        "    instance_count=1,\n",
        "    request_settings=OnlineRequestSettings(\n",
        "        request_timeout_ms=180000,\n",
        "        max_concurrent_requests_per_instance=1,\n",
        "        max_queue_wait_ms=180000,\n",
        "    ),\n",
        ")\n",
        "ml_client.begin_create_or_update(deploy).result()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "<h5> Change the traffic </h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1760506127418
        }
      },
      "outputs": [],
      "source": [
        "# 3) Traffic routing\n",
        "ep = ml_client.online_endpoints.get(endpoint_name)\n",
        "ep.traffic = {\"blue\": 100}\n",
        "ml_client.begin_create_or_update(ep).result()\n",
        "\n",
        "print(\"endpoint:\", endpoint_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "<h5> Send Request </h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1760526142244
        }
      },
      "outputs": [],
      "source": [
        "# Test UCF101 inference endpoint using test_payload format\n",
        "import json\n",
        "import base64\n",
        "import requests\n",
        "from pathlib import Path\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "\n",
        "# Get endpoint credentials\n",
        "keys = ml_client.online_endpoints.get_keys(name=endpoint_name).primary_key\n",
        "scoring_url = ml_client.online_endpoints.get(endpoint_name).scoring_uri\n",
        "headers = {\"Authorization\": f\"Bearer {keys}\", \"Content-Type\": \"application/json\"}\n",
        "\n",
        "# Load test example from UCF101 test set\n",
        "test_jsonl = Path(\"./converted_ucf101/ucf101_test.jsonl\")\n",
        "image_dir = Path(\"./converted_ucf101/images\")\n",
        "\n",
        "if test_jsonl.exists():\n",
        "    with open(test_jsonl, 'r', encoding='utf-8') as f:\n",
        "        test_example = json.loads(f.readline())\n",
        "    \n",
        "    # Build test_payload (same format as local test)\n",
        "    test_payload = {\n",
        "        \"image_paths\": [str(image_dir / img_path) for img_path in test_example['conversations'][0]['images']],\n",
        "        \"prompt\": test_example['conversations'][0]['user'],\n",
        "        \"max_new_tokens\": 50\n",
        "    }\n",
        "    \n",
        "    print(f\"[INFO] Test payload prepared:\")\n",
        "    print(f\"  - Images: {len(test_payload['image_paths'])} frames\")\n",
        "    print(f\"  - Ground truth: {test_example['conversations'][0]['assistant']}\")\n",
        "    \n",
        "    # Convert image_paths to base64 for endpoint request\n",
        "    encoded_images = []\n",
        "    for img_path in test_payload[\"image_paths\"]:\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        buffer = BytesIO()\n",
        "        img.save(buffer, format=\"JPEG\")\n",
        "        img_b64 = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
        "        encoded_images.append(img_b64)\n",
        "    \n",
        "    # Build endpoint payload (convert image_paths -> images with base64)\n",
        "    endpoint_payload = {\n",
        "        \"images\": encoded_images,  # base64-encoded images instead of paths\n",
        "        \"prompt\": test_payload[\"prompt\"],\n",
        "        \"max_new_tokens\": test_payload[\"max_new_tokens\"]\n",
        "    }\n",
        "    \n",
        "    print(f\"\\n[INFO] Sending request with {len(encoded_images)} frames\")\n",
        "    \n",
        "    # Send request\n",
        "    res = requests.post(scoring_url, headers=headers, json=endpoint_payload, timeout=300)\n",
        "    print(f\"[INFO] Status code: {res.status_code}\")\n",
        "    \n",
        "    if res.status_code == 200:\n",
        "        result = res.json()\n",
        "        if isinstance(result, str):\n",
        "            result = json.loads(result)\n",
        "        \n",
        "        print(\"\\n[RESULT]\")\n",
        "        print(f\"  Predicted class: {result.get('predicted_class')}\")\n",
        "        print(f\"  Raw output: {result.get('raw_output')}\")\n",
        "        print(f\"  Match: {result.get('predicted_class') == test_example['conversations'][0]['assistant']}\")\n",
        "    else:\n",
        "        print(f\"[ERROR] {res.text}\")\n",
        "else:\n",
        "    print(\"[WARN] Test file not found. Cannot send request.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "<h5> Delete Endpoint </h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1760526519374
        }
      },
      "outputs": [],
      "source": [
        "ml_client.online_endpoints.begin_delete(name=endpoint_name).wait()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "<h5> Problem Detection </h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1760507636862
        }
      },
      "outputs": [],
      "source": [
        "print(res.status_code)\n",
        "print(res.headers.get(\"content-type\"))\n",
        "print(res.text[:1000])  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1760507641435
        }
      },
      "outputs": [],
      "source": [
        "ml_client.online_deployments.get_logs(\n",
        "    name=\"blue\", endpoint_name=endpoint_name, lines=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "<h2> End of Script </h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "<h3>Appendix</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h5> Batch Evaluation </h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1760514068557
        }
      },
      "outputs": [],
      "source": [
        "# Evaluate endpoint on multiple test samples\n",
        "import json\n",
        "import base64\n",
        "import requests\n",
        "from pathlib import Path\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import time\n",
        "\n",
        "# Load test dataset\n",
        "test_jsonl = Path(\"./converted_ucf101/ucf101_test.jsonl\")\n",
        "image_dir = Path(\"./converted_ucf101/images\")\n",
        "\n",
        "if not test_jsonl.exists():\n",
        "    print(\"[ERROR] Test file not found\")\n",
        "else:\n",
        "    # Load all test examples\n",
        "    with open(test_jsonl, 'r', encoding='utf-8') as f:\n",
        "        test_examples = [json.loads(line) for line in f]\n",
        "    \n",
        "    print(f\"[INFO] Loaded {len(test_examples)} test samples\")\n",
        "    \n",
        "    # Evaluate on first N samples (adjust as needed)\n",
        "    num_samples = min(10, len(test_examples))  # Start with 10 samples\n",
        "    \n",
        "    results = []\n",
        "    correct = 0\n",
        "    total_inference_time = 0\n",
        "    errors_count = 0\n",
        "    \n",
        "    print(f\"\\n[INFO] Evaluating {num_samples} samples...\")\n",
        "    print(f\"Progress: \", end='', flush=True)\n",
        "    \n",
        "    for i, example in enumerate(test_examples[:num_samples]):\n",
        "        print(f\"{i+1}/{num_samples}...\", end=' ', flush=True)\n",
        "        \n",
        "        # Build payload\n",
        "        image_paths = [str(image_dir / img_path) for img_path in example['conversations'][0]['images']]\n",
        "        ground_truth = example['conversations'][0]['assistant']\n",
        "        \n",
        "        # Convert to base64\n",
        "        encoded_images = []\n",
        "        try:\n",
        "            for img_path in image_paths:\n",
        "                img = Image.open(img_path).convert(\"RGB\")\n",
        "                buffer = BytesIO()\n",
        "                img.save(buffer, format=\"JPEG\")\n",
        "                img_b64 = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
        "                encoded_images.append(img_b64)\n",
        "        except Exception as e:\n",
        "            print(f\"\\n[ERROR] Sample {i}: Image loading failed - {str(e)}\")\n",
        "            errors_count += 1\n",
        "            results.append({\n",
        "                'id': example['id'],\n",
        "                'ground_truth': ground_truth,\n",
        "                'predicted': None,\n",
        "                'correct': False,\n",
        "                'error': f\"Image loading: {str(e)}\"\n",
        "            })\n",
        "            continue\n",
        "        \n",
        "        endpoint_payload = {\n",
        "            \"images\": encoded_images,\n",
        "            \"prompt\": example['conversations'][0]['user'],\n",
        "            \"max_new_tokens\": 50\n",
        "        }\n",
        "        \n",
        "        # Send request\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            res = requests.post(scoring_url, headers=headers, json=endpoint_payload, timeout=300)\n",
        "            inference_time = time.time() - start_time\n",
        "            \n",
        "            if res.status_code == 200:\n",
        "                result = res.json()\n",
        "                if isinstance(result, str):\n",
        "                    result = json.loads(result)\n",
        "                \n",
        "                predicted_class = result.get('predicted_class')\n",
        "                is_correct = (predicted_class == ground_truth)\n",
        "                \n",
        "                if is_correct:\n",
        "                    correct += 1\n",
        "                \n",
        "                results.append({\n",
        "                    'id': example['id'],\n",
        "                    'ground_truth': ground_truth,\n",
        "                    'predicted': predicted_class,\n",
        "                    'correct': is_correct,\n",
        "                    'inference_time': inference_time\n",
        "                })\n",
        "                \n",
        "                total_inference_time += inference_time\n",
        "            else:\n",
        "                print(f\"\\n[ERROR] Sample {i}: HTTP {res.status_code} - {res.text[:100]}\")\n",
        "                errors_count += 1\n",
        "                results.append({\n",
        "                    'id': example['id'],\n",
        "                    'ground_truth': ground_truth,\n",
        "                    'predicted': None,\n",
        "                    'correct': False,\n",
        "                    'error': f\"HTTP {res.status_code}: {res.text[:200]}\"\n",
        "                })\n",
        "        \n",
        "        except requests.exceptions.Timeout:\n",
        "            inference_time = time.time() - start_time\n",
        "            print(f\"\\n[ERROR] Sample {i}: Request timeout after {inference_time:.1f}s\")\n",
        "            errors_count += 1\n",
        "            results.append({\n",
        "                'id': example['id'],\n",
        "                'ground_truth': ground_truth,\n",
        "                'predicted': None,\n",
        "                'correct': False,\n",
        "                'error': f\"Timeout after {inference_time:.1f}s\"\n",
        "            })\n",
        "        \n",
        "        except Exception as e:\n",
        "            print(f\"\\n[ERROR] Sample {i}: {type(e).__name__} - {str(e)}\")\n",
        "            errors_count += 1\n",
        "            results.append({\n",
        "                'id': example['id'],\n",
        "                'ground_truth': ground_truth,\n",
        "                'predicted': None,\n",
        "                'correct': False,\n",
        "                'error': str(e)\n",
        "            })\n",
        "    \n",
        "    print(\"\\n\")  # New line after progress\n",
        "    \n",
        "    # Calculate metrics\n",
        "    successful_samples = num_samples - errors_count\n",
        "    accuracy = correct / successful_samples if successful_samples > 0 else 0\n",
        "    avg_inference_time = total_inference_time / successful_samples if successful_samples > 0 else 0\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"EVALUATION RESULTS\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Total samples: {num_samples}\")\n",
        "    print(f\"Successful requests: {successful_samples}\")\n",
        "    print(f\"Failed requests: {errors_count}\")\n",
        "    print(f\"Correct predictions: {correct}\")\n",
        "    print(f\"Accuracy: {accuracy:.2%} ({correct}/{successful_samples})\")\n",
        "    print(f\"Average inference time: {avg_inference_time:.2f} seconds\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Show sample results\n",
        "    print(f\"\\nSample Results (first 5):\")\n",
        "    for r in results[:5]:\n",
        "        status = \"✓\" if r['correct'] else \"✗\"\n",
        "        pred_str = str(r.get('predicted', 'ERROR'))\n",
        "        time_str = f\"{r.get('inference_time', 0):.1f}s\" if 'inference_time' in r else \"N/A\"\n",
        "        print(f\"  {status} GT: {r['ground_truth']:20s} | Pred: {pred_str:20s} | Time: {time_str}\")\n",
        "    \n",
        "    # Confusion analysis\n",
        "    if correct < successful_samples:\n",
        "        print(f\"\\nIncorrect Predictions:\")\n",
        "        for r in results:\n",
        "            if not r['correct'] and 'error' not in r:\n",
        "                print(f\"  GT: {r['ground_truth']:20s} → Pred: {str(r.get('predicted', 'None'))}\")\n",
        "    \n",
        "    # Error summary\n",
        "    if errors_count > 0:\n",
        "        print(f\"\\nError Summary:\")\n",
        "        error_types = {}\n",
        "        for r in results:\n",
        "            if 'error' in r:\n",
        "                error_key = r['error'].split(':')[0][:30]  # First 30 chars of error\n",
        "                error_types[error_key] = error_types.get(error_key, 0) + 1\n",
        "        for error_type, count in error_types.items():\n",
        "            print(f\"  {error_type}: {count} occurrences\")\n",
        "    \n",
        "    print(f\"\\n[INFO] Evaluation completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h5>Trackio</h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed Trackio data review (alternative to UI)\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "conn = sqlite3.connect(\"./ft-project.db\")\n",
        "\n",
        "# Retrieve all metrics\n",
        "metrics_df = pd.read_sql(\"SELECT * FROM metrics ORDER BY step;\", conn)\n",
        "\n",
        "print(f\"[INFO] Total logged steps: {len(metrics_df)}\")\n",
        "print(f\"[INFO] Run name: {metrics_df['run_name'].unique()}\")\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"TRAINING METRICS\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "# Parse metrics\n",
        "for idx, row in metrics_df.iterrows():\n",
        "    step = row['step']\n",
        "    timestamp = row['timestamp']\n",
        "    metrics_blob = row['metrics']\n",
        "    \n",
        "    # Convert binary data to JSON\n",
        "    try:\n",
        "        metrics_dict = json.loads(metrics_blob.decode('utf-8') if isinstance(metrics_blob, bytes) else metrics_blob)\n",
        "        \n",
        "        print(f\"\\nStep {step} ({timestamp}):\")\n",
        "        \n",
        "        # Display key metrics\n",
        "        important_keys = ['train/loss', 'train/learning_rate', 'train/epoch', \n",
        "                         'train/global_step', 'train/grad_norm']\n",
        "        \n",
        "        for key in important_keys:\n",
        "            if key in metrics_dict:\n",
        "                print(f\"  {key:30s}: {metrics_dict[key]:.6f}\")\n",
        "        \n",
        "        # GPU metrics (if pynvml is available)\n",
        "        gpu_keys = [k for k in metrics_dict.keys() if 'gpu' in k.lower() or 'memory' in k.lower()]\n",
        "        if gpu_keys:\n",
        "            print(f\"\\n  GPU Metrics:\")\n",
        "            for key in gpu_keys[:5]:  # Display first 5 only\n",
        "                print(f\"    {key:28s}: {metrics_dict[key]}\")\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"\\n[ERROR] Step {step}: Cannot parse metrics - {str(e)[:100]}\")\n",
        "\n",
        "conn.close()\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"[INFO] Use trackio.show(project='ft-project') to view interactive UI\")\n",
        "print(f\"[INFO] Or open browser: http://127.0.0.1:7860/?project=ft-project\")\n",
        "print(f\"{'='*80}\")"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
